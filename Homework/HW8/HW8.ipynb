{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a Bayesian mixture model, the posterior predictive distribution represents our uncertainty about future observations given the data we have. It's created by integrating over all possible parameter values, weighted by how likely those parameter values are according to the posterior distribution. To draw from the posterior predictive distribution, we perform the following\n",
    "\n",
    "- Draw samples from the posterior distribution of the parameters. The sampling methods used for this purpose are Markov Chain Monte Carlo (MCMC) methods like HMC or Gibbs, which allow us to approximate the posterior distribution when we can't compute it analytically. Since mixture models involve multiple distributions, we sample the parameters for each component of the mixture, including the mixing proportions (weights).\n",
    "- For each sample of the parameters, we draw new data observations. For a mixture model, the parameters include both the component-specific parameters (such as means and variances in a Gaussian mixture model) and the mixing coefficients that determine the weight of each component in the mixture. To generate a point we select a component from the mixture according to the sampled mixing proportions and then with the selected component, we draw a data point from the component distribution characterized by the sampled parameters.\n",
    "- Aggregate (e.g. via averaging or taking quantiles) these generated data points to form the posterior predictive distribution. This distribution reflects the compounded uncertainty of both the parameters and the randomness of the data generating process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The posterior predictive distribution for a new observation $x_{\\text{new}}$ is calculated by integrating over the entire parameter space $\\Theta$, taking into account the probability of each parameter value $\\theta$ given the observed data $D$.\n",
    "\n",
    "$p(x_{\\text{new}} | D) = \\int_{\\Theta} p(x_{\\text{new}} | \\theta) p(\\theta | D) d\\theta$\n",
    "\n",
    "\n",
    " - $p(x_{\\text{new}} | \\theta)$ is the likelihood of the new data point $x_{\\text{new}}$ given the parameter $\\theta$, which is based on the chosen model.\n",
    " - $p(\\theta | D)$ is the posterior distribution of the parameters given the data $D$, encapsulating our updated beliefs about the parameters after seeing the data.\n",
    " -  The integral $\\int_{\\Theta}$ over $\\theta$ sums the likelihoods of the new observation across all possible values of $\\theta$, weighted by the posterior probability of each $\\theta$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When data are missing at random (MAR), their absence does not substantially affect the conclusions drawn from the analysis, allowing us to exclude them without biasing results. In contrast, for data missing not at random (MNAR), we can adjust for potential biases by incorporating a latent variable that indicates whether data points are missing or observed, with values of 0 for missing and 1 for observed. The interesting scenario to address is when data are missing completely at random (MCAR). In such cases, after determining the priors from the data we do have, we can use those priors to generate new, hypothetical data. This process creates what is known as the posterior predictive distribution. By applying this distribution and employing Markov Chain Monte Carlo (MCMC) sampling techniques, we are able to estimate and fill in the missing values based on the observed data and any additional information we have about these missing points. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
